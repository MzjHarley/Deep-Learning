{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e31dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5a77d",
   "metadata": {},
   "source": [
    "## 卷积层实现\n",
    "### 自定义权值\n",
    "在 TensorFlow 中，通过 tf.nn.conv2d 函数可以方便地实现 2D 卷积运算。tf.nn.conv2d基于输入𝑿: [b,ℎ ,𝑤 ,𝑐𝑖𝑛] 和卷积核𝑾: [𝑘 𝑘 𝑐𝑖𝑛 𝑐𝑜𝑢𝑡] 进行卷积运算，得到输出𝑶 [b,ℎ′, 𝑤′ ,𝑐𝑜𝑢𝑡] ，其中𝑐𝑖𝑛表示输入通道数，𝑐𝑜𝑢𝑡表示卷积核的数量，也是输出特征图的通道数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c6081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2,5,5,3])\n",
    "w = tf.random.normal([3,3,3,4])\n",
    "out = tf.nn.conv2d(x,w,strides=1,padding=[[0,0],[0,0],[0,0],[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e01c78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 3, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b0834b",
   "metadata": {},
   "source": [
    "其中 padding 参数的设置格式为：padding=[[0,0],[上,下],[左,右],[0,0]]  \n",
    "上下左右各填充一个单位，则 padding 参数设置为[[0,0],[1,1],[1,1],[0,0]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93254ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2,5,5,3])\n",
    "w = tf.random.normal([3,3,3,4])\n",
    "out = tf.nn.conv2d(x,w,strides=1,padding=[[0,0],[1,1],[1,1],[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11efeea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 5, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5834746",
   "metadata": {},
   "source": [
    "特别地，通过设置参数 padding='SAME'、strides=1 可以直接得到输入、输出同大小的卷积层，其中 padding 的具体数量由 TensorFlow 自动计算并完成填充操作."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d121f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([2,5,5,3])\n",
    "w = tf.random.normal([3,3,3,4])\n",
    "out = tf.nn.conv2d(x,w,strides=1,padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d353d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 5, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e321842",
   "metadata": {},
   "source": [
    "卷积神经网络层与全连接层一样，可以设置网络带偏置向量。tf.nn.conv2d 函数是没有实现偏置向量计算的，添加偏置只需要手动累加偏置张量即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1921152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据[cout]格式创建偏置向量\n",
    "b = tf.zeros([4])\n",
    "# 在卷积输出上叠加偏置向量，它会自动 broadcasting 为[b,h',w',cout]\n",
    "out = out + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b2ea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([4]), TensorShape([2, 5, 5, 4]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape,out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f6249",
   "metadata": {},
   "source": [
    "### 卷积层类\n",
    "通过卷积层类 layers.Conv2D 可以**不需要手动定义卷积核𝑾和偏置𝒃张量**，直接调用类实例即可完成卷积层的前向计算，实现更加高层和快捷。  \n",
    "在 TensorFlow 中，API 的命名有一定的规律，首字母大写的对象一般表示类，全部小写的一般表示函数，如 **layers.Conv2D表示卷积层类，nn.conv2d表示卷积运算函数.**   \n",
    "使用类方式会(在创建类时或 build 时)**自动创建需要的权值张量和偏置向量等**，用户不需要记忆卷积核张量的定义格式，因此使用起来更简单方便，但是灵活性也略低。   \n",
    "函数方式的接口需要**自行定义权值和偏置等**，更加灵活和底层。  \n",
    "在新建卷积层类时，只需要指定卷积核数量参数 filters，卷积核大小 kernel_size，步长strides，填充 padding 等即可,如果卷积核高宽不等，步长行列方向不等，此时需要将 kernel_size 参数设计为 tuple格式(𝑘ℎ 𝑘𝑤)，strides 参数设计为(𝑠ℎ 𝑠𝑤)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71765d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Conv2D(4,kernel_size=3,strides=1,padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd8cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = tf.keras.layers.Conv2D(4,kernel_size=(3,4),strides=(2,1),padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adb15c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5, 5, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Conv2D(4,kernel_size=3,strides=1,padding='SAME')\n",
    "out = layer(x)\n",
    "out.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9edb491",
   "metadata": {},
   "source": [
    "在类 Conv2D 中，保存了卷积核张量𝑾和偏置𝒃，可以通过类成员 trainable_variables直接返回𝑾和𝒃的列表。  \n",
    "也可以直接调用类实例 layer.kernel、layer.bias名访问𝑾和𝒃张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97cafde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 3, 4) dtype=float32, numpy=\n",
       " array([[[[ 0.2437422 ,  0.08774582,  0.15351939,  0.24705073],\n",
       "          [ 0.11227664, -0.1016742 ,  0.21263763, -0.30423364],\n",
       "          [-0.18269694, -0.02163401, -0.21902698, -0.091611  ]],\n",
       " \n",
       "         [[-0.02625018, -0.14880483, -0.22838715,  0.18353176],\n",
       "          [ 0.2334849 ,  0.15451592,  0.23177841, -0.23397374],\n",
       "          [-0.15307483,  0.01141182, -0.15656859, -0.1619232 ]],\n",
       " \n",
       "         [[ 0.17415318,  0.0223144 ,  0.09880814, -0.05945921],\n",
       "          [ 0.07118085, -0.06688489,  0.2255921 , -0.14549907],\n",
       "          [ 0.20877329, -0.2085037 ,  0.14816236, -0.1297698 ]]],\n",
       " \n",
       " \n",
       "        [[[-0.2613098 ,  0.30639586,  0.13460332,  0.2691286 ],\n",
       "          [-0.23557824,  0.0474596 ,  0.04440111, -0.20931974],\n",
       "          [ 0.21925685, -0.29571152,  0.29577765, -0.01720574]],\n",
       " \n",
       "         [[-0.16052338, -0.130909  ,  0.28106228,  0.14111361],\n",
       "          [-0.1240409 ,  0.06575584,  0.11045045, -0.23101526],\n",
       "          [ 0.12061578,  0.1554558 ,  0.23825678,  0.29665998]],\n",
       " \n",
       "         [[-0.20595032, -0.28479415,  0.21073672, -0.04049569],\n",
       "          [-0.09561649, -0.08149864,  0.02590725, -0.04055825],\n",
       "          [-0.19548523,  0.05881712,  0.02421233,  0.01933691]]],\n",
       " \n",
       " \n",
       "        [[[ 0.04745966, -0.16311869, -0.1134294 , -0.09648831],\n",
       "          [-0.00444004, -0.27664226,  0.15664017,  0.12200832],\n",
       "          [ 0.0159575 ,  0.06895036, -0.20601544, -0.25856617]],\n",
       " \n",
       "         [[ 0.081738  ,  0.03241041,  0.0255487 , -0.22721365],\n",
       "          [ 0.19766071, -0.27789176, -0.1496375 , -0.00796455],\n",
       "          [-0.27141663, -0.20500919, -0.11896743,  0.294695  ]],\n",
       " \n",
       "         [[-0.25872326,  0.03124258,  0.25344595,  0.05357331],\n",
       "          [ 0.04341364, -0.07973573, -0.25841004, -0.00712931],\n",
       "          [ 0.0388141 , -0.06763104, -0.24229941, -0.02341738]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'conv2d_2/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd5179e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 3, 4) dtype=float32, numpy=\n",
       " array([[[[ 0.2437422 ,  0.08774582,  0.15351939,  0.24705073],\n",
       "          [ 0.11227664, -0.1016742 ,  0.21263763, -0.30423364],\n",
       "          [-0.18269694, -0.02163401, -0.21902698, -0.091611  ]],\n",
       " \n",
       "         [[-0.02625018, -0.14880483, -0.22838715,  0.18353176],\n",
       "          [ 0.2334849 ,  0.15451592,  0.23177841, -0.23397374],\n",
       "          [-0.15307483,  0.01141182, -0.15656859, -0.1619232 ]],\n",
       " \n",
       "         [[ 0.17415318,  0.0223144 ,  0.09880814, -0.05945921],\n",
       "          [ 0.07118085, -0.06688489,  0.2255921 , -0.14549907],\n",
       "          [ 0.20877329, -0.2085037 ,  0.14816236, -0.1297698 ]]],\n",
       " \n",
       " \n",
       "        [[[-0.2613098 ,  0.30639586,  0.13460332,  0.2691286 ],\n",
       "          [-0.23557824,  0.0474596 ,  0.04440111, -0.20931974],\n",
       "          [ 0.21925685, -0.29571152,  0.29577765, -0.01720574]],\n",
       " \n",
       "         [[-0.16052338, -0.130909  ,  0.28106228,  0.14111361],\n",
       "          [-0.1240409 ,  0.06575584,  0.11045045, -0.23101526],\n",
       "          [ 0.12061578,  0.1554558 ,  0.23825678,  0.29665998]],\n",
       " \n",
       "         [[-0.20595032, -0.28479415,  0.21073672, -0.04049569],\n",
       "          [-0.09561649, -0.08149864,  0.02590725, -0.04055825],\n",
       "          [-0.19548523,  0.05881712,  0.02421233,  0.01933691]]],\n",
       " \n",
       " \n",
       "        [[[ 0.04745966, -0.16311869, -0.1134294 , -0.09648831],\n",
       "          [-0.00444004, -0.27664226,  0.15664017,  0.12200832],\n",
       "          [ 0.0159575 ,  0.06895036, -0.20601544, -0.25856617]],\n",
       " \n",
       "         [[ 0.081738  ,  0.03241041,  0.0255487 , -0.22721365],\n",
       "          [ 0.19766071, -0.27789176, -0.1496375 , -0.00796455],\n",
       "          [-0.27141663, -0.20500919, -0.11896743,  0.294695  ]],\n",
       " \n",
       "         [[-0.25872326,  0.03124258,  0.25344595,  0.05357331],\n",
       "          [ 0.04341364, -0.07973573, -0.25841004, -0.00712931],\n",
       "          [ 0.0388141 , -0.06763104, -0.24229941, -0.02341738]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'conv2d_2/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.kernel,layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d63b3",
   "metadata": {},
   "source": [
    "## BatchNorm层\n",
    "卷积神经网络的出现，网络参数量大大减低，使得几十层的深层网络成为可能。然而，在残差网络出现之前，网络的加深使得网络训练变得非常不稳定，甚至出现网络长时间不更新甚至不收敛的现象，同时网络对超参数比较敏感，超参数的微量扰动也会导致网络的训练轨迹完全改变。  \n",
    "2015 年，Google 研究人员 Sergey Ioffe 等提出了一种参数标准化(Normalize)的手段，并基于参数标准化设计了 Batch Nomalization(简写为 BatchNorm，或 BN)层 。**BN 层的提出，使得网络的超参数的设定更加自由，比如更大的学习率、更随意的网络初始化等，同时网络的收敛速度更快，性能也更好,使得神经网络的损失函数空间更加平滑，梯度下降不容易陷入局部极值，不那么依赖权重初始化；也使得参数更新时梯度的取值范围更小，梯度更新更具可预测性，不容易出现梯度爆炸和梯度消失.** BN 层提出后便广泛地应用在各种深度网络模型上，卷积层、BN 层、ReLU 层、池化层一度成为网络模型的标配单元块，通过堆叠 Conv-BN-ReLU-Pooling 方式往往可以获得不错的模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d9162",
   "metadata": {},
   "source": [
    "网络层输入𝑥分布相近，并且分布在较小范围内时(如 0 附近)，更有利于函数的优化。那么如何保证输入𝑥的分布相近呢？数据标准化可以实现此目的，通过数据标准化操作可以将数据𝑥映射到𝑥̂：  \n",
    "$𝑥̂=\\frac{x-\\mu_r}{\\sqrt{\\sigma_r^2+\\epsilon}}$  \n",
    "其中$𝜇_𝑟、𝜎_𝑟^2$来自统计的所有数据的均值和方差，𝜖是为防止出现除 0 错误而设置的较小数字，如 1e − 8。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f918c",
   "metadata": {},
   "source": [
    "在基于 Batch 的训练阶段,考虑Batch 内部的均值$𝜇_𝐵$和方差$𝜎_𝐵^2$：    \n",
    "$𝑥̂_{train}=\\frac{x_{train} ~ ~ ~ - ~ ~ \\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}}$  \n",
    "并记录每个 Batch 的统计数据$𝜇_𝐵$,$𝜎_𝐵^2$，用于统计真实的全局$𝜇_𝑟、𝜎_𝑟^2$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ea73aa",
   "metadata": {},
   "source": [
    "在测试阶段，根据记录的每个 Batch 的$𝜇_𝐵$,$𝜎_𝐵^2$估计出**所有训练数据**的$𝜇_𝑟、𝜎_𝑟^2$，将每层输入标准化：  \n",
    "$𝑥̂_{test}=\\frac{x_{test} ~ ~ ~ - ~ ~ \\mu_r}{\\sqrt{\\sigma_r^2+\\epsilon}}$  \n",
    "上述的标准化运算并没有引入额外的待优化变量，$𝜇_𝑟、𝜎_𝑟^2$和$𝜇_𝐵$,$𝜎_𝐵^2$均由统计得到，不需要参与梯度更新。  \n",
    "为什么测试阶段要使用全局的均值和方差呢？  \n",
    "某一个样本经过测试时应该有确定的输出，如果在测试时也是用测试数据的means和var，那么样本的输出会随所处batch的不同，而有所差异。即batch的随机性导致了样本测试的不确定性。所以使用固定的在训练中得出的mean和var。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515ef359",
   "metadata": {},
   "source": [
    "上述的标准化运算并没有引入额外的待优化变量,$𝜇_𝐵$,$𝜎_𝐵^2$均由统计得到，不需要参与梯度更新。实际上，为了提高 BN 层的表达能力，BN 层作者引入了“scale and shift”技巧，将𝑥̂变量再次映射变换：  \n",
    "$𝑥̃ = 𝑥̂ ∙ 𝛾 + \\beta$  \n",
    "其中𝛾参数实现对标准化后的𝑥̂再次进行缩放，𝛽参数实现对标准化的𝑥̂进行平移，不同的是，𝛾、𝛽参数均由反向传播算法自动优化，实现网络层“按需”缩放平移数据的分布的目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2355ee0",
   "metadata": {},
   "source": [
    "**如何得到全局的均值和方差呢？**  \n",
    "根据训练时得到得到的每个batch的$𝜇_𝐵$,$𝜎_𝐵^2$，通过滑动平均的思想迭代更新得到全局的均值和方差：  \n",
    "$𝜇_𝑟 = momentum  ∙ 𝜇_𝑟 + (1 − momentum) ∙ 𝜇_𝐵$  \n",
    "$𝜎_𝑟^2 = momentum ∙ 𝜎_𝑟^2+(1 − momentum) ∙ 𝜎_𝐵^2$  \n",
    "$𝜇_𝑟,𝜎_𝑟^2$初始值分别为0，1，在\n",
    "TensorFlow 中，momentum 默认设置为 0.99。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb8797",
   "metadata": {},
   "source": [
    "需要注意的是，对于 2D 特征图输入𝑿:[b , ℎ 𝑤 𝑐 ],BN 层并不是计算每个点的$𝜇_𝐵$,$𝜎_𝐵^2$，而是在通道轴𝑐上面统计每个通道上面所有数据的$𝜇_𝐵$,$𝜎_𝐵^2$，因此$𝜇_𝐵$,$𝜎_𝐵^2$是每个通道上所有其它维度的均值和方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c3a5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.00423183,  0.00289833, -0.0021715 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(102400, 3), dtype=float32, numpy=\n",
       " array([[-0.01069316, -1.4869391 ,  0.06559213],\n",
       "        [-0.7466509 ,  0.14568272,  0.47498414],\n",
       "        [ 1.3210725 , -1.8251016 ,  2.7887805 ],\n",
       "        ...,\n",
       "        [-0.6645756 , -1.6973752 ,  0.21304633],\n",
       "        [ 0.4367766 ,  0.01970497, -1.5768087 ],\n",
       "        [ 0.5880983 , -0.03935365,  0.17863801]], dtype=float32)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=tf.random.normal([100,32,32,3])\n",
    "x=tf.reshape(x,[-1,3])\n",
    "ub=tf.reduce_mean(x,axis=0)\n",
    "ub,x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ae60f",
   "metadata": {},
   "source": [
    "在 TensorFlow 中，通过 tf.keras.layers.BatchNormalization()类可以非常方便地实现 BN 层."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32cffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer=tf.keras.layers.BatchNormalization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246674b",
   "metadata": {},
   "source": [
    "与全连接层、卷积层不同，BN 层的训练阶段和测试阶段的行为不同，需要通过设置training 标志位来区分训练模式还是测试模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb38d8",
   "metadata": {},
   "source": [
    "## 卷积变种\n",
    "### 空洞卷积\n",
    "普通的卷积层为了减少网络的参数量，卷积核的设计通常选择较小的 1×1 和3 × 3感受野大小。小卷积核使得网络提取特征时的感受野区域有限，但是增大感受野的区域又会增加网络的参数量和计算代价，因此需要权衡设计。  \n",
    "空洞卷积(Dilated/Atrous Convolution)的提出较好地解决这个问题，空洞卷积在普通卷积的感受野上增加一个 Dilation Rate 参数，用于控制感受野区域的采样步长.**尽管 Dilation Rate 的增大会使得感受野区域增大，但是实际参与运算的点数仍然保持不变**。  \n",
    "空洞卷积在不增加网络参数的条件下，提供了更大的感受野窗口。但是在使用空洞卷积设置网络模型时，需要精心设计 Dilation Rate 参数来避免出现网格效应，同时较大的Dilation Rate 参数并不利于小物体的检测、语义分割等任务。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a979d55",
   "metadata": {},
   "source": [
    "在 TensorFlow 中，可以通过设置 layers.Conv2D()类的 dilation_rate 参数来选择使用普通卷积还是空洞卷积。  \n",
    "当 dilation_rate 参数设置为默认值 1 时，使用普通卷积方式进行运算；当 dilation_rate 参数大于 1 时，采样空洞卷积方式进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac3981c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 3, 3, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.random.normal([1,7,7,1])\n",
    "layer = tf.keras.layers.Conv2D(1,kernel_size=3,strides=1,dilation_rate=2)\n",
    "out = layer(x) \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef7cac",
   "metadata": {},
   "source": [
    "### 转置卷积\n",
    "通过在输入之间填充大量的 padding 来实现输出高宽大于输入高宽的效果，从而实现向上采样的目的."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105160b",
   "metadata": {},
   "source": [
    "  \n",
    "![jupyter](https://github.com/MzjHarley/Tensorflow2.x.x/blob/main/Convolutional%20Neural%20Network/Photo/1.png)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5306a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\n",
       "array([[[[ -67.],\n",
       "         [ -77.]],\n",
       "\n",
       "        [[-117.],\n",
       "         [-127.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(25)+1\n",
    "x = tf.reshape(x,[1,5,5,1])\n",
    "x = tf.cast(x, tf.float32)\n",
    "w = tf.constant([[-1,2,-3.],[4,-5,6],[-7,8,-9]])\n",
    "w = tf.expand_dims(w,axis=2)\n",
    "w = tf.expand_dims(w,axis=3)\n",
    "print(w.shape)#[h,w,c,cout]\n",
    "out = tf.nn.conv2d(x,w,strides=2,padding='VALID')\n",
    "# we can use this:x->x'[1,25],w->w'(sparse matrix)[4,25] to explain, the result is w'@the Transpose of x' ,reshape to [2,2].\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171445b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 5, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.nn.conv2d_transpose(out, w, strides=2,padding='VALID',output_shape=[1,5,5,1])\n",
    "# we can use this:out->out'[4,1],w->w'(sparse matrix)[4,25] to explain, the result is the transpose of out'@w' ,reshape to [5,5].\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3f837",
   "metadata": {},
   "source": [
    "在使用 tf.nn.conv2d_transpose 进行转置卷积运算时，需要额外手动设置输出的高宽。tf.nn.conv2d_transpose 并不支持自定义 padding 设置，只能设置为 'VALID' 或者 'SAME'。当设置 padding=’VALID’时，输出大小表达为：  \n",
    "𝑜 = (𝑖 − 1)𝑠 + k  \n",
    "当设置 padding=’SAME’时，输出大小表达为：  \n",
    "𝑜 = 𝑖 ∙ s  \n",
    "i 为转置卷积输入，s为步长，k为转置卷积核大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a1240",
   "metadata": {},
   "source": [
    "转置卷积也可以和其他层一样，通过 tf.keraslayers.Conv2DTranspose 类创建一个转置卷积层，然后调用实例即可完成前向计算."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c8f3a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 4, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Conv2DTranspose(2,kernel_size=3,strides=1,padding='VALID')\n",
    "x2 = layer(out) \n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1316b",
   "metadata": {},
   "source": [
    "### 分离卷积\n",
    "卷积核的每个通道与输入的每个通道进行卷积运算，得到多个通道的中间特征。这个多通道的中间特征张量接下来进行多个1×1卷积核的普通卷积运算，得到多个高宽不变的输出，这些输出在通道轴上面进行拼接，从而产生最终的分离卷积层的输出。可以看到，分离卷积层包含了两步卷积运算，第一步卷积运算是单个卷积核，第二个卷积运算包含了多个卷积核。  \n",
    "分离卷积有什么优势呢？一个很明显的优势在于，同样的输入和输出，采用Separable Convolution 的参数量约是普通卷积的$\\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a0a15",
   "metadata": {},
   "source": [
    "![jupyter](https://github.com/MzjHarley/Tensorflow2.x.x/blob/main/Convolutional%20Neural%20Network/Photo/2.png)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:code] *",
   "language": "python",
   "name": "conda-env-code-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
